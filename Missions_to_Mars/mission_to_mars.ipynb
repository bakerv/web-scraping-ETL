{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "d6dfefb6-a8d4-4868-b6c0-c387d6c67cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import requests\n",
    "from splinter import Browser\n",
    "import pymongo\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "dab27a02-bcd8-41be-9f2a-0b9b72900ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#urls to scrape\n",
    "nasamarsurl = \"https://mars.nasa.gov/news/\"\n",
    "nasajplurl = \"https://www.jpl.nasa.gov/spaceimages/?search=&category=Mars\"\n",
    "sfurl = \"https://space-facts.com/mars/\"\n",
    "usgsurl = \"https://astrogeology.usgs.gov/search/results?q=hemisphere+enhanced&k1=target&v1=Mars\"\n",
    "\n",
    "# connection path for splinter browser\n",
    "driverpath = {'executable_path':'Resources/chromedriver.exe'}\n",
    "\n",
    "#connection to mongodb\n",
    "conn = 'mongodb://localhost:27017'\n",
    "client = pymongo.MongoClient(conn)\n",
    "\n",
    "#mongodb database paths\n",
    "nasamars_collection = client.mission_to_mars.nasamars\n",
    "jplimages_collection = client.mission_to_mars.jplimages\n",
    "spacefacts_collection = client.mission_to_mars.spacefacts\n",
    "usgsimages_collection = client.mission_to_mars.usgsimages\n",
    "scrape_collection = client.mission_to_mars.scrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "cc440be3-26a7-4439-96e7-1ea606f162b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pull_data(scrapeurl):\n",
    "    \"\"\"\n",
    "    Access the specified url, and return the html contents of the entire page\n",
    "        Args:\n",
    "            scrapeurl (str): the URL of the page you wish to scrape \n",
    "    \"\"\"\n",
    "    browser = Browser('chrome',**driverpath, headless=False)\n",
    "    browser.visit(scrapeurl)\n",
    "    rawdata = browser.html\n",
    "    browser.quit()\n",
    "\n",
    "    return rawdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "fd154c15-9831-4e45-9b6d-60f6e36d24b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nasamars_scraper(scrapeurl,collection):\n",
    "    '''\n",
    "    Access the Nasa Mars News Site and return the article titles, summarys, and dates in dictionary format\n",
    "        Args:\n",
    "            scrapeurl (str): url of the site to be scraped\n",
    "            collection (obj): mongodb collection to save the data to, connected using MongoClient\n",
    "    '''\n",
    "    def clean_data(rawdata,collection):\n",
    "        # clear database to prevent duplicate entries\n",
    "        collection.drop()\n",
    "\n",
    "        #parse the sites html code into a searchable list\n",
    "        soup = bs(rawdata, 'html.parser')\n",
    "\n",
    "        # put the html structure for each article into a list\n",
    "        content = (soup\n",
    "                    .find('section', class_='grid_gallery module list_view')\n",
    "                    .find_all('div', class_= 'list_text'))\n",
    "\n",
    "        for entry in content:\n",
    "        # iterate through the list and return desired attributes for each article\n",
    "            try:\n",
    "                #extract desired attributes\n",
    "                title = (entry.find('a').text)\n",
    "                summary = (entry.find('div', class_= 'article_teaser_body').text)\n",
    "                date = (entry.find('div', class_ = 'list_date').text)\n",
    "                url = entry.a['href']\n",
    "\n",
    "                #save to mongoDB\n",
    "                document = {\n",
    "                    'title': title,\n",
    "                    'summary': summary,\n",
    "                    'date': date,\n",
    "                    'url': url\n",
    "                }\n",
    "                collection.insert_one(document)\n",
    "\n",
    "            except Exception as error:\n",
    "                print(error)\n",
    "                \n",
    "    rawdata = pull_data(scrapeurl)\n",
    "    clean_data(rawdata,collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "34309b80-fe6a-443f-b194-685048d0da6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def usgs_scraper(scrapeurl,collection):\n",
    "    '''\n",
    "    Access composite mars hemisphere images obtained from the usgs\n",
    "        Args:\n",
    "            scrapeurl (str): url of the site to be scraped\n",
    "            collection (obj): mongodb collection to save the data to, connected using MongoClient\n",
    "    '''\n",
    "    # parse the html code to locate URLs for each desired page\n",
    "    def find_subpages(rawdata):\n",
    "\n",
    "        soup = bs(rawdata, 'html.parser')\n",
    "        subpages = soup.find_all('div', class_='item')\n",
    "        links = [page.a['href'] for page in subpages]\n",
    "        \n",
    "        return links\n",
    "    \n",
    "    # parse through the html code to locate the desired information\n",
    "    def clean_subpages(rawdata):\n",
    "        \n",
    "        soup = bs(rawdata, 'html.parser')\n",
    "        download_url = soup.find('div', class_='content').a['href']\n",
    "        img_url = soup.find('div', class_='downloads').a['href']\n",
    "        title = soup.h2.text\n",
    "        \n",
    "        return [title,img_url,download_url]\n",
    "    \n",
    "    # Add information from each subpage to the document\n",
    "    def load_data(links,collection):\n",
    "        #clear database to prevent duplicate entries\n",
    "        collection.drop()\n",
    "        \n",
    "        #create a list containing all subpage information\n",
    "        document = []\n",
    "        for link in links:\n",
    "            try:\n",
    "                rawdata = pull_data('http://astrogeology.usgs.gov' + link)\n",
    "                cleandata = clean_subpages(rawdata)\n",
    "                img_dict = {\n",
    "                    'title': cleandata[0],\n",
    "                    'img_url': cleandata[1],\n",
    "                    'dowloand_url': cleandata[2]\n",
    "                }\n",
    "                document.append(img_dict)\n",
    "                \n",
    "            except Exception as error:\n",
    "                print(error)\n",
    "                \n",
    "        #save to mongoDB       \n",
    "        collection.insert_one({'USGS_Mars_Hemispheres':document})\n",
    "        \n",
    "        return document\n",
    "                \n",
    "    rawdata = pull_data(scrapeurl)\n",
    "    links = find_subpages(rawdata)\n",
    "    load_data(links,collection)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "bea060bb-037c-4c77-a3c0-10f81d71d282",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sf_scraper(scrapeurl,collection):\n",
    "    '''\n",
    "    Access mars data tables from spacefacts.com\n",
    "        Args:\n",
    "            scrapeurl (str): url of the site to be scraped\n",
    "            collection (obj): mongodb collection to save the data to, connected using MongoClient\n",
    "    '''\n",
    "    # retrieve data from source and transform into a dictionary\n",
    "    def extract_data(scrapeurl):\n",
    "        # extract datatables from url\n",
    "        rawdata = pd.read_html(scrapeurl)\n",
    "        \n",
    "        #transform first table into a dictionary\n",
    "        sf_dict = {}\n",
    "        for row,column in rawdata[0].iterrows():\n",
    "            sf_dict[column[0]] = column[1]\n",
    "            \n",
    "        return sf_dict\n",
    "    \n",
    "    # save dictionary to mongoDB\n",
    "    def load_data(document,collection):\n",
    "        collection.drop()\n",
    "        collection.insert_one(document)\n",
    "        \n",
    "    sf_dict = extract_data(scrapeurl)\n",
    "    load_data(sf_dict,collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "9957be36-56ca-4462-9dc4-e16eaac38bf4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def scrape(collection):\n",
    "    '''\n",
    "    Run all scraping funcitons, and save the data to a single dictionary\n",
    "        Args:\n",
    "            collection (obj): mongodb collection to save the data to, connected using MongoClient\n",
    "    '''\n",
    "    #Scrape data and store it into mongoDB\n",
    "    nasamars_scraper(nasamarsurl,nasamars_collection)\n",
    "    usgs_scraper(usgsurl,usgsimages_collection)\n",
    "    sf_scraper(sfurl,spacefacts_collection)\n",
    "    \n",
    "    #Read data from each collection into memory\n",
    "    nasamars_content = [x for x in nasamars_collection.find()]\n",
    "    usgs_content = [x for x in usgsimages_collection.find()]\n",
    "    sf_content = [x for x in spacefacts_collection.find()]\n",
    "    \n",
    "    #store all data in a single dictionary\n",
    "    document = {'Nasa_Mars_Exploration_Articles': nasamars_content,\n",
    "                   'USGS_Mars_Hemisphere_Images': usgs_content,\n",
    "                   'Space_Facts_Table': sf_content}\n",
    "\n",
    "    #Save data as a mongoDB database\n",
    "    collection.drop()\n",
    "    collection.insert_one(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "47b02f97-19c2-4b00-b1f9-b6112e4b1909",
   "metadata": {},
   "outputs": [],
   "source": [
    "scrape(scrape_collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734bb9e9-68bf-4486-bb53-1c37ac9cbc38",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
