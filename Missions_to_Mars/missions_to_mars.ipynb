{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d6dfefb6-a8d4-4868-b6c0-c387d6c67cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import requests\n",
    "from splinter import Browser\n",
    "import pymongo\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "dab27a02-bcd8-41be-9f2a-0b9b72900ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#urls to scrape\n",
    "nasamarsurl = \"https://mars.nasa.gov/news/\"\n",
    "nasajplurl = \"https://www.jpl.nasa.gov/spaceimages/?search=&category=Mars\"\n",
    "sfurl = \"https://space-facts.com/mars/\"\n",
    "usgsurl = \"https://astrogeology.usgs.gov/search/results?q=hemisphere+enhanced&k1=target&v1=Mars\"\n",
    "\n",
    "# connect to the web with splinter\n",
    "driverpath = {'executable_path':'Resources/chromedriver.exe'}\n",
    "\n",
    "#connect to mongodb\n",
    "conn = 'mongodb://localhost:27017'\n",
    "client = pymongo.MongoClient(conn)\n",
    "\n",
    "#database paths\n",
    "nasamars_collection = client.mission_to_mars.nasamars\n",
    "jplimages_collection = client.mission_to_mars.jplimages\n",
    "spacefacts_collection = client.mission_to_mars.spacefacts\n",
    "usgsimages_collection = client.mission_to_mars.usgsimages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7cc927a0-b925-42ba-b6f5-5131d288a67e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-77f50ab9924c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msoup\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'html.parser'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m content = (soup\n\u001b[0;32m      3\u001b[0m            .find('div', class_=\"col-span-3\"))\n\u001b[0;32m      4\u001b[0m            \u001b[1;31m#.find_all('div', class_= 'list_text'))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#display(soup)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'test' is not defined"
     ]
    }
   ],
   "source": [
    "soup = bs(test, 'html.parser')\n",
    "content = (soup\n",
    "           .find('div', class_=\"col-span-3\"))\n",
    "           #.find_all('div', class_= 'list_text'))\n",
    "#display(soup)\n",
    "display(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a853263-d591-4b8f-9b18-114e34491e27",
   "metadata": {},
   "outputs": [
    {
     "ename": "ElementDoesNotExist",
     "evalue": "no elements could be found with name \"Galleries\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\splinter\\element_list.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m     41\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_container\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     43\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mElementDoesNotExist\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-e46f4e9f409c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mbrowser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBrowser\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'chrome'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mdriverpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheadless\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mbrowser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvisit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnasajplurl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mbrowser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Galleries\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mrawdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbrowser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhtml\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mbrowser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mquit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\splinter\\driver\\webdriver\\__init__.py\u001b[0m in \u001b[0;36mcheck\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    617\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    618\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcheck\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 619\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_by_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfirst\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    620\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    621\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0muncheck\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\splinter\\element_list.py\u001b[0m in \u001b[0;36mfirst\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     55\u001b[0m             \u001b[1;33m>>\u001b[0m\u001b[1;33m>\u001b[0m \u001b[1;32massert\u001b[0m \u001b[0melement_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0melement_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfirst\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m         \"\"\"\n\u001b[1;32m---> 57\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     58\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\splinter\\element_list.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m     42\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_container\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m             raise ElementDoesNotExist(\n\u001b[0m\u001b[0;32m     45\u001b[0m                 u'no elements could be found with {0} \"{1}\"'.format(\n\u001b[0;32m     46\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_by\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mquery\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mElementDoesNotExist\u001b[0m: no elements could be found with name \"Galleries\""
     ]
    }
   ],
   "source": [
    "browser = Browser('chrome',**driverpath, headless=False)\n",
    "browser.visit(nasajplurl)\n",
    "browser.check(\"Galleries\")\n",
    "rawdata = browser.html\n",
    "browser.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc440be3-26a7-4439-96e7-1ea606f162b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pull_data(scrapeurl):\n",
    "\n",
    "    browser = Browser('chrome',**driverpath, headless=False)\n",
    "    browser.visit(scrapeurl)\n",
    "    browser.find_by_name('Galleries')\n",
    "    rawdata = browser.html\n",
    "    browser.quit()\n",
    "\n",
    "    return rawdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd154c15-9831-4e45-9b6d-60f6e36d24b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nasamars_scraper(scrapeurl,collection):\n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    #scrape NASA Mars News articles\n",
    "    def pull_data(scrapeurl):\n",
    "       \n",
    "        browser = Browser('chrome',**driverpath, headless=False)\n",
    "        browser.visit(scrapeurl)\n",
    "        rawdata = browser.html\n",
    "        browser.quit()\n",
    "        \n",
    "        return rawdata\n",
    "    \n",
    "    def clean_data(rawdata,collection):\n",
    "        # clear database to prevent duplicate entries\n",
    "        collection.drop()\n",
    "\n",
    "        #parse the sites html code into a searchable list\n",
    "        soup = bs(rawdata, 'html.parser')\n",
    "\n",
    "        # put the html structure for each article into a list\n",
    "        content = (soup\n",
    "                    .find('section', class_='grid_gallery module list_view')\n",
    "                    .find_all('div', class_= 'list_text'))\n",
    "\n",
    "        for entry in content:\n",
    "        # iterate through the list and return desired attributes for each article\n",
    "            try:\n",
    "                #extract desired attributes\n",
    "                title = (entry.find('a').text)\n",
    "                summary = (entry.find('div', class_= 'article_teaser_body').text)\n",
    "                date = (entry.find('div', class_ = 'list_date').text)\n",
    "                url = entry.find('a','href')\n",
    "\n",
    "                #save to mongoDB\n",
    "                document = {\n",
    "                    'title': title,\n",
    "                    'summary': summary,\n",
    "                    'date': date,\n",
    "                    'url': url\n",
    "                }\n",
    "    \n",
    "                collection.insert_one(document)\n",
    "\n",
    "            except Exception as error:\n",
    "                print(error)\n",
    "                \n",
    "    rawdata = pull_data(scrapeurl)\n",
    "    clean_data(rawdata,collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8254259b-fd4a-43da-bd4f-73b0a611bd5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "nasamars_scraper(nasamarsurl,nasamars_collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "34309b80-fe6a-443f-b194-685048d0da6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def usgs_scraper(scrapeurl,collection):\n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    #scrape usgs for links to hi resolution images\n",
    "    def pull_data(scrapeurl):\n",
    "       \n",
    "        browser = Browser('chrome',**driverpath, headless=False)\n",
    "        browser.visit(scrapeurl)\n",
    "        rawdata = browser.html\n",
    "        browser.quit()\n",
    "        \n",
    "        return rawdata\n",
    "    \n",
    "    def clean_data(rawdata,collection):\n",
    "        # clear database to prevent duplicate entries\n",
    "        collection.drop()\n",
    "\n",
    "        #parse the sites html code into a searchable list\n",
    "        soup = bs(rawdata, 'html.parser')\n",
    "\n",
    "        # put the html structure for each article into a list\n",
    "        content = (soup\n",
    "                    .find('section', class_='grid_gallery module list_view')\n",
    "                    .find_all('div', class_= 'list_text'))\n",
    "\n",
    "        for entry in content:\n",
    "        # iterate through the list and return desired attributes for each article\n",
    "            try:\n",
    "                #extract desired attributes\n",
    "                title = (entry.find('a').text)\n",
    "                summary = (entry.find('div', class_= 'article_teaser_body').text)\n",
    "                date = (entry.find('div', class_ = 'list_date').text)\n",
    "                url = entry.find('a','href')\n",
    "\n",
    "                #save to mongoDB\n",
    "                document = {\n",
    "                    'title': title,\n",
    "                    'summary': summary,\n",
    "                    'date': date,\n",
    "                    'url': url\n",
    "                }\n",
    "                collection.insert_one(document)\n",
    "\n",
    "            except Exception as error:\n",
    "                print(error)\n",
    "                \n",
    "    rawdata = pull_data(scrapeurl)\n",
    "    clean_data(rawdata,collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "acbc688c-7156-441d-8a62-20306e4fbbc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "browser = Browser('chrome',**driverpath, headless=False)\n",
    "browser.visit(usgsurl)\n",
    "rawdata = browser.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3e1a3540-5c4c-4a83-8d75-f756d869b0d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/search/map/Mars/Viking/cerberus_enhanced'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup = bs(rawdata, 'html.parser')\n",
    "links = soup.find_all('div', class_='item')\n",
    "links[0].a['href']\n",
    "test = []\n",
    "[test.append(item.a['href']) for item in links]\n",
    "test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "e4ccb5da-8401-4aee-8640-a810f6dd2a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "browser = Browser('chrome',**driverpath, headless=False)\n",
    "baseurl =\"http://astrogeology.usgs.gov\"\n",
    "\n",
    "browser.visit(baseurl + test[0])\n",
    "testdata = browser.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "ac75df10-8205-4301-a867-9bb391643c55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://astropedia.astrogeology.usgs.gov/download/Mars/Viking/cerberus_enhanced.tif/full.jpg'"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup = bs(testdata, 'html.parser')\n",
    "highreslink = soup.find('div', class_='downloads').a['href']\n",
    "\n",
    "highreslink"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bea060bb-037c-4c77-a3c0-10f81d71d282",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sf_scraper(scrapeurl,collection):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    # retrieve data from source and transform into a dictionary\n",
    "    def extract_data(scrapeurl):\n",
    "        # extract datatables from url\n",
    "        rawdata = pd.read_html(scrapeurl)\n",
    "        \n",
    "        #transform first table into a dictionary\n",
    "        sf_dict = {}\n",
    "        for row,column in rawdata[0].iterrows():\n",
    "            sf_dict[column[0]] = column[1]\n",
    "            \n",
    "        return sf_dict\n",
    "    \n",
    "    # save dictionary to mongoDB\n",
    "    def load_data(document,collection):    \n",
    "        collection.insert_one(document)\n",
    "        \n",
    "    sf_dict = extract_data(scrapeurl)\n",
    "    load_data(sf_dict,collection)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "87b22fc6-20aa-48b8-bd4b-3b627f22bd09",
   "metadata": {},
   "outputs": [],
   "source": [
    "sf_scraper(sfurl,spacefacts_collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "739fd894-cdd6-4433-8da4-b225edd3cbe2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Equatorial Diameter:</td>\n",
       "      <td>6,792 km</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Polar Diameter:</td>\n",
       "      <td>6,752 km</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mass:</td>\n",
       "      <td>6.39 × 10^23 kg (0.11 Earths)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Moons:</td>\n",
       "      <td>2 (Phobos &amp; Deimos)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Orbit Distance:</td>\n",
       "      <td>227,943,824 km (1.38 AU)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Orbit Period:</td>\n",
       "      <td>687 days (1.9 years)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Surface Temperature:</td>\n",
       "      <td>-87 to -5 °C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>First Record:</td>\n",
       "      <td>2nd millennium BC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Recorded By:</td>\n",
       "      <td>Egyptian astronomers</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      0                              1\n",
       "0  Equatorial Diameter:                       6,792 km\n",
       "1       Polar Diameter:                       6,752 km\n",
       "2                 Mass:  6.39 × 10^23 kg (0.11 Earths)\n",
       "3                Moons:            2 (Phobos & Deimos)\n",
       "4       Orbit Distance:       227,943,824 km (1.38 AU)\n",
       "5         Orbit Period:           687 days (1.9 years)\n",
       "6  Surface Temperature:                   -87 to -5 °C\n",
       "7         First Record:              2nd millennium BC\n",
       "8          Recorded By:           Egyptian astronomers"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rawdata = pd.read_html(sfurl)\n",
    "marstable = rawdata[0]\n",
    "marstable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "81f8e998-d9bb-4a02-a05e-18f725015199",
   "metadata": {},
   "outputs": [],
   "source": [
    "sf_dict = {}\n",
    "for row,column in rawdata[0].iterrows():\n",
    "    sf_dict[column[0]] = column[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6551b37f-6b25-44e8-a9cd-1f089e50736c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Equatorial Diameter:': '6,792 km',\n",
       " 'Polar Diameter:': '6,752 km',\n",
       " 'Mass:': '6.39 × 10^23 kg (0.11 Earths)',\n",
       " 'Moons:': '2 (Phobos & Deimos)',\n",
       " 'Orbit Distance:': '227,943,824 km (1.38 AU)',\n",
       " 'Orbit Period:': '687 days (1.9 years)',\n",
       " 'Surface Temperature:': '-87 to -5 °C',\n",
       " 'First Record:': '2nd millennium BC',\n",
       " 'Recorded By:': 'Egyptian astronomers'}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sf_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94cb8ec7-82a3-4097-ab22-2028f9bcd4e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
