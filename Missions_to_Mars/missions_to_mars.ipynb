{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d6dfefb6-a8d4-4868-b6c0-c387d6c67cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import requests\n",
    "from splinter import Browser\n",
    "import pymongo\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "dab27a02-bcd8-41be-9f2a-0b9b72900ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#urls to scrape\n",
    "nasamarsurl = \"https://mars.nasa.gov/news/\"\n",
    "nasajplurl = \"https://www.jpl.nasa.gov/spaceimages/?search=&category=Mars\"\n",
    "sfurl = \"https://space-facts.com/mars/\"\n",
    "usgsurl = \"https://astrogeology.usgs.gov/search/results?q=hemisphere+enhanced&k1=target&v1=Mars\"\n",
    "\n",
    "# connect to the web with splinter\n",
    "driverpath = {'executable_path':'Resources/chromedriver.exe'}\n",
    "\n",
    "#connect to mongodb\n",
    "conn = 'mongodb://localhost:27017'\n",
    "client = pymongo.MongoClient(conn)\n",
    "\n",
    "#database paths\n",
    "nasamars_collection = client.mission_to_mars.nasamars\n",
    "jplimages_collection = client.mission_to_mars.jplimages\n",
    "spacefacts_collection = client.mission_to_mars.spacefacts\n",
    "usgsimages_collection = client.mission_to_mars.usgsimages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7cc927a0-b925-42ba-b6f5-5131d288a67e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-77f50ab9924c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msoup\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'html.parser'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m content = (soup\n\u001b[0;32m      3\u001b[0m            .find('div', class_=\"col-span-3\"))\n\u001b[0;32m      4\u001b[0m            \u001b[1;31m#.find_all('div', class_= 'list_text'))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#display(soup)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'test' is not defined"
     ]
    }
   ],
   "source": [
    "soup = bs(test, 'html.parser')\n",
    "content = (soup\n",
    "           .find('div', class_=\"col-span-3\"))\n",
    "           #.find_all('div', class_= 'list_text'))\n",
    "#display(soup)\n",
    "display(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a853263-d591-4b8f-9b18-114e34491e27",
   "metadata": {},
   "outputs": [
    {
     "ename": "ElementDoesNotExist",
     "evalue": "no elements could be found with name \"Galleries\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\splinter\\element_list.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m     41\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_container\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     43\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mElementDoesNotExist\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-e46f4e9f409c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mbrowser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBrowser\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'chrome'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mdriverpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheadless\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mbrowser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvisit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnasajplurl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mbrowser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Galleries\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mrawdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbrowser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhtml\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mbrowser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mquit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\splinter\\driver\\webdriver\\__init__.py\u001b[0m in \u001b[0;36mcheck\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    617\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    618\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcheck\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 619\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_by_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfirst\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    620\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    621\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0muncheck\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\splinter\\element_list.py\u001b[0m in \u001b[0;36mfirst\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     55\u001b[0m             \u001b[1;33m>>\u001b[0m\u001b[1;33m>\u001b[0m \u001b[1;32massert\u001b[0m \u001b[0melement_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0melement_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfirst\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m         \"\"\"\n\u001b[1;32m---> 57\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     58\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\splinter\\element_list.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m     42\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_container\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m             raise ElementDoesNotExist(\n\u001b[0m\u001b[0;32m     45\u001b[0m                 u'no elements could be found with {0} \"{1}\"'.format(\n\u001b[0;32m     46\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_by\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mquery\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mElementDoesNotExist\u001b[0m: no elements could be found with name \"Galleries\""
     ]
    }
   ],
   "source": [
    "browser = Browser('chrome',**driverpath, headless=False)\n",
    "browser.visit(nasajplurl)\n",
    "browser.check(\"Galleries\")\n",
    "rawdata = browser.html\n",
    "browser.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc440be3-26a7-4439-96e7-1ea606f162b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pull_data(scrapeurl):\n",
    "\n",
    "    browser = Browser('chrome',**driverpath, headless=False)\n",
    "    browser.visit(scrapeurl)\n",
    "    browser.find_by_name('Galleries')\n",
    "    rawdata = browser.html\n",
    "    browser.quit()\n",
    "\n",
    "    return rawdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd154c15-9831-4e45-9b6d-60f6e36d24b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nasamars_scraper(scrapeurl,collection):\n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    #scrape NASA Mars News articles\n",
    "    def pull_data(scrapeurl):\n",
    "       \n",
    "        browser = Browser('chrome',**driverpath, headless=False)\n",
    "        browser.visit(scrapeurl)\n",
    "        rawdata = browser.html\n",
    "        browser.quit()\n",
    "        \n",
    "        return rawdata\n",
    "    \n",
    "    def clean_data(rawdata,collection):\n",
    "        # clear database to prevent duplicate entries\n",
    "        collection.drop()\n",
    "\n",
    "        #parse the sites html code into a searchable list\n",
    "        soup = bs(rawdata, 'html.parser')\n",
    "\n",
    "        # put the html structure for each article into a list\n",
    "        content = (soup\n",
    "                    .find('section', class_='grid_gallery module list_view')\n",
    "                    .find_all('div', class_= 'list_text'))\n",
    "\n",
    "        for entry in content:\n",
    "        # iterate through the list and return desired attributes for each article\n",
    "            try:\n",
    "                #extract desired attributes\n",
    "                title = (entry.find('a').text)\n",
    "                summary = (entry.find('div', class_= 'article_teaser_body').text)\n",
    "                date = (entry.find('div', class_ = 'list_date').text)\n",
    "                url = entry.find('a','href')\n",
    "\n",
    "                #save to mongoDB\n",
    "                document = {\n",
    "                    'title': title,\n",
    "                    'summary': summary,\n",
    "                    'date': date,\n",
    "                    'url': url\n",
    "                }\n",
    "    \n",
    "                collection.insert_one(document)\n",
    "\n",
    "            except Exception as error:\n",
    "                print(error)\n",
    "                \n",
    "    rawdata = pull_data(scrapeurl)\n",
    "    clean_data(rawdata,collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8254259b-fd4a-43da-bd4f-73b0a611bd5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "nasamars_scraper(nasamarsurl,nasamars_collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "34309b80-fe6a-443f-b194-685048d0da6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def usgs_scraper(scrapeurl,collection):\n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    #scrape usgs for links to hi resolution images\n",
    "    def pull_data(scrapeurl):\n",
    "       \n",
    "        browser = Browser('chrome',**driverpath, headless=False)\n",
    "        browser.visit(scrapeurl)\n",
    "        rawdata = browser.html\n",
    "        browser.quit()\n",
    "        \n",
    "        return rawdata\n",
    "    \n",
    "    def find_subpages(rawdata):\n",
    "        # clear database to prevent duplicate entries\n",
    "        collection.drop()\n",
    "\n",
    "        #parse the sites html code into a searchable list\n",
    "        soup = bs(rawdata, 'html.parser')\n",
    "        subpages = soup.find_all('div', class_='item')\n",
    "        \n",
    "        links = []\n",
    "        [links.append(page.a['href']) for page in subpages]\n",
    "        \n",
    "        return links\n",
    "    \n",
    "    def clean_subpages(rawdata):\n",
    "        \n",
    "        soup = bs(rawdata, 'html.parser')\n",
    "        download_url = soup.find('div', class_='content').a['href']\n",
    "        img_url = soup.find('div', class_='downloads').a['href']\n",
    "        title = soup.h2.text\n",
    "        \n",
    "        return [title,img_url,download_url]\n",
    "        \n",
    "    def load_data(links,collection):\n",
    "                \n",
    "        document = []\n",
    "        \n",
    "        for link in links:\n",
    "            try:\n",
    "                rawdata = pull_data('http://astrogeology.usgs.gov' + link)\n",
    "                cleandata = clean_subpages(rawdata)\n",
    "                \n",
    "                # Add information from the subpage to the document\n",
    "                img_dict = {\n",
    "                    'title': cleandata[0],\n",
    "                    'img_url': cleandata[1],\n",
    "                    'dowloand_url': cleandata[2]\n",
    "                }\n",
    "                document.append(img_dict)\n",
    "                \n",
    "            except Exception as error:\n",
    "                print(error)\n",
    "                \n",
    "        #save to mongoDB       \n",
    "        collection.insert_one({'USGS_Mars_Hemispheres':document})\n",
    "                \n",
    "    rawdata = pull_data(scrapeurl)\n",
    "    links = find_subpages(rawdata)\n",
    "    load_data(links,collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "f638e69e-4fa6-410d-91fe-e0dec7215744",
   "metadata": {},
   "outputs": [],
   "source": [
    "usgs_scraper(usgsurl,usgsimages_collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "bea060bb-037c-4c77-a3c0-10f81d71d282",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sf_scraper(scrapeurl,collection):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    # retrieve data from source and transform into a dictionary\n",
    "    def extract_data(scrapeurl):\n",
    "        # extract datatables from url\n",
    "        rawdata = pd.read_html(scrapeurl)\n",
    "        \n",
    "        #transform first table into a dictionary\n",
    "        sf_dict = {}\n",
    "        for row,column in rawdata[0].iterrows():\n",
    "            sf_dict[column[0]] = column[1]\n",
    "            \n",
    "        return sf_dict\n",
    "    \n",
    "    # save dictionary to mongoDB\n",
    "    def load_data(document,collection):    \n",
    "        collection.insert_one(document)\n",
    "        \n",
    "    sf_dict = extract_data(scrapeurl)\n",
    "    load_data(sf_dict,collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "87b22fc6-20aa-48b8-bd4b-3b627f22bd09",
   "metadata": {},
   "outputs": [],
   "source": [
    "sf_scraper(sfurl,spacefacts_collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94cb8ec7-82a3-4097-ab22-2028f9bcd4e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
